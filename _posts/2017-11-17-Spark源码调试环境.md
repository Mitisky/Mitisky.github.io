---
layout: post
title:  "IDEA构建SPARK源码"
date:   2017-11-17  +0800
categories: big data
---

#IDEA构建SPARK源码
两种构建方法，分别是maven和sbt。这里采用maven。
我的环境是
 
- macOS Sierra
- maven 3.5.0
- scala 2.11
- spark code 2.11
- jdk8
- Idea 2017

scala，java和maven首先肯定要安装好，详细安装步骤网上很多资料。
下面所有操作都是在Idea中完成

 - 点击[File]->[Open]选择你的spark跟路径。或者在控制台中cd到跟路径下执行[mvn idea:idea]，然后就能构造一个idea的project工程。这里强烈推荐给maven加一个国内的mirror。
 - 等全部完成后，用idea再build的话。可能会报某些class找不到，而且工程中的确没有这些class。这些是avro对象，代码是需要生成的。我这里遇到的模块是Flume Sink，打开Maven Projects窗口，在右击Flume Sink模块，选择[Generate sources and update folder]。现在利用Idea进行Build应该是没有问题的。
 - 最后进行调试，这里还要做一些小修改。这里执行的example模块下的JavaSparkPi类。可以先运行一下试试。我遇到的是报错是某些ClassNotFound的异常，这个我问题找了好久才解决。把Project structure打开。切到examples模块下的dependencies标签页。会发现很多依赖的scope是有问题的，需要都改成compile，不然编译过程中难免报类不存在了。直接到pom文件中将scope都改成compile。
 - 要能执行还需要添加一个参数，否则执行肯定会报出找不到Master。打开[Run/Debug Configurations]在[VM options]中加上[-Dspark.master=local]当前执行的是本地模式。现在就可以执行调试代码了。